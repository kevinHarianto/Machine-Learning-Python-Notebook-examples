{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b615a0",
   "metadata": {},
   "source": [
    "Download the \"spam.csv\" dataset and load it into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "496fb2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df= pd.read_csv(\"E:/downld/spam.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108ba7c",
   "metadata": {},
   "source": [
    "Import the required libraries: pandas, scikit-learn, and numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbab1960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycaret\n",
      "  Downloading pycaret-3.3.2-py3-none-any.whl (486 kB)\n",
      "     -------------------------------------- 486.1/486.1 kB 6.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from pycaret) (5.5.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-win_amd64.whl (29 kB)\n",
      "Collecting schemdraw==0.15\n",
      "  Downloading schemdraw-0.15-py3-none-any.whl (106 kB)\n",
      "     -------------------------------------- 106.8/106.8 kB 6.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: statsmodels>=0.12.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from pycaret) (0.13.2)\n",
      "Requirement already satisfied: ipywidgets>=7.6.5 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from pycaret) (7.6.5)\n",
      "Requirement already satisfied: requests>=2.27.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from pycaret) (2.28.1)\n",
      "Collecting deprecation>=2.1.0\n",
      "  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: scipy<=1.11.4,>=1.6.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from pycaret) (1.9.1)\n",
      "Requirement already satisfied: numpy<1.27,>=1.21 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from pycaret) (1.21.5)\n",
      "Collecting pmdarima>=2.0.4\n",
      "  Downloading pmdarima-2.0.4-cp39-cp39-win_amd64.whl (614 kB)\n",
      "     -------------------------------------- 615.0/615.0 kB 9.6 MB/s eta 0:00:00\n",
      "Collecting scikit-plot>=0.3.7\n",
      "  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
      "Collecting imbalanced-learn>=0.12.0\n",
      "  Downloading imbalanced_learn-0.12.3-py3-none-any.whl (258 kB)\n",
      "     -------------------------------------- 258.3/258.3 kB 8.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numba>=0.55.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from pycaret) (0.55.1)\n",
      "Requirement already satisfied: tqdm>=4.62.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from pycaret) (4.64.1)\n",
      "Requirement already satisfied: pandas<2.2.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from pycaret) (1.4.4)\n",
      "Collecting yellowbrick>=1.4\n",
      "  Downloading yellowbrick-1.5-py3-none-any.whl (282 kB)\n",
      "     -------------------------------------- 282.6/282.6 kB 8.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: markupsafe>=2.0.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from pycaret) (2.0.1)\n",
      "Collecting sktime==0.26.0\n",
      "  Downloading sktime-0.26.0-py3-none-any.whl (21.8 MB)\n",
      "     ---------------------------------------- 21.8/21.8 MB 8.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: psutil>=5.9.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from pycaret) (5.9.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from pycaret) (2.0.0)\n",
      "Collecting tbats>=1.1.3\n",
      "  Downloading tbats-1.1.3-py3-none-any.whl (44 kB)\n",
      "     ---------------------------------------- 44.0/44.0 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: matplotlib<3.8.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from pycaret) (3.5.2)\n",
      "Collecting pyod>=1.1.3\n",
      "  Downloading pyod-2.0.1.tar.gz (163 kB)\n",
      "     -------------------------------------- 163.8/163.8 kB 5.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: ipython>=5.5.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from pycaret) (7.31.1)\n",
      "Collecting jinja2>=3\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "     -------------------------------------- 133.3/133.3 kB 8.2 MB/s eta 0:00:00\n",
      "Collecting kaleido>=0.2.1\n",
      "  Downloading kaleido-0.2.1-py2.py3-none-win_amd64.whl (65.9 MB)\n",
      "     ---------------------------------------- 65.9/65.9 MB 7.7 MB/s eta 0:00:00\n",
      "Collecting plotly-resampler>=0.8.3.1\n",
      "  Downloading plotly_resampler-0.10.0-py3-none-any.whl (80 kB)\n",
      "     ---------------------------------------- 80.7/80.7 kB 4.7 MB/s eta 0:00:00\n",
      "Collecting scikit-learn>1.4.0\n",
      "  Downloading scikit_learn-1.5.1-cp39-cp39-win_amd64.whl (11.0 MB)\n",
      "     ---------------------------------------- 11.0/11.0 MB 8.4 MB/s eta 0:00:00\n",
      "Collecting category-encoders>=2.4.0\n",
      "  Downloading category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
      "     ---------------------------------------- 81.9/81.9 kB 4.5 MB/s eta 0:00:00\n",
      "Collecting joblib<1.4,>=1.2.0\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "     -------------------------------------- 302.2/302.2 kB 9.4 MB/s eta 0:00:00\n",
      "Collecting lightgbm>=3.0.0\n",
      "  Downloading lightgbm-4.5.0-py3-none-win_amd64.whl (1.4 MB)\n",
      "     ---------------------------------------- 1.4/1.4 MB 9.2 MB/s eta 0:00:00\n",
      "Collecting plotly>=5.14.0\n",
      "  Downloading plotly-5.23.0-py3-none-any.whl (17.3 MB)\n",
      "     ---------------------------------------- 17.3/17.3 MB 9.0 MB/s eta 0:00:00\n",
      "Collecting importlib-metadata>=4.12.0\n",
      "  Downloading importlib_metadata-8.2.0-py3-none-any.whl (25 kB)\n",
      "Collecting scikit-learn>1.4.0\n",
      "  Downloading scikit_learn-1.4.2-cp39-cp39-win_amd64.whl (10.6 MB)\n",
      "     ---------------------------------------- 10.6/10.6 MB 9.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from sktime==0.26.0->pycaret) (21.3)\n",
      "Collecting scikit-base<0.8.0\n",
      "  Downloading scikit_base-0.7.8-py3-none-any.whl (130 kB)\n",
      "     -------------------------------------- 130.1/130.1 kB 7.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: patsy>=0.5.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from category-encoders>=2.4.0->pycaret) (0.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from imbalanced-learn>=0.12.0->pycaret) (2.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.12.0->pycaret) (3.8.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from ipython>=5.5.0->pycaret) (0.18.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from ipython>=5.5.0->pycaret) (63.4.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from ipython>=5.5.0->pycaret) (0.1.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from ipython>=5.5.0->pycaret) (5.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from ipython>=5.5.0->pycaret) (0.4.5)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from ipython>=5.5.0->pycaret) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from ipython>=5.5.0->pycaret) (5.1.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from ipython>=5.5.0->pycaret) (0.2.0)\n",
      "Requirement already satisfied: pygments in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from ipython>=5.5.0->pycaret) (2.11.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from ipython>=5.5.0->pycaret) (3.0.20)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from ipywidgets>=7.6.5->pycaret) (1.0.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from ipywidgets>=7.6.5->pycaret) (0.2.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from ipywidgets>=7.6.5->pycaret) (3.5.2)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from ipywidgets>=7.6.5->pycaret) (6.15.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib<3.8.0->pycaret) (9.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib<3.8.0->pycaret) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib<3.8.0->pycaret) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib<3.8.0->pycaret) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib<3.8.0->pycaret) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from matplotlib<3.8.0->pycaret) (2.8.2)\n",
      "Requirement already satisfied: jupyter_core in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->pycaret) (4.11.1)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->pycaret) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->pycaret) (4.16.0)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from numba>=0.55.0->pycaret) (0.38.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from pandas<2.2.0->pycaret) (2022.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from plotly>=5.14.0->pycaret) (8.0.1)\n",
      "Collecting dash>=2.9.0\n",
      "  Downloading dash-2.17.1-py3-none-any.whl (7.5 MB)\n",
      "     ---------------------------------------- 7.5/7.5 MB 9.0 MB/s eta 0:00:00\n",
      "Collecting tsdownsample>=0.1.3\n",
      "  Downloading tsdownsample-0.1.3-cp39-none-win_amd64.whl (923 kB)\n",
      "     -------------------------------------- 923.5/923.5 kB 7.3 MB/s eta 0:00:00\n",
      "Collecting orjson<4.0.0,>=3.8.0\n",
      "  Downloading orjson-3.10.6-cp39-none-win_amd64.whl (136 kB)\n",
      "     -------------------------------------- 136.2/136.2 kB 7.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from pmdarima>=2.0.4->pycaret) (0.29.32)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from pmdarima>=2.0.4->pycaret) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from requests>=2.27.1->pycaret) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from requests>=2.27.1->pycaret) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from requests>=2.27.1->pycaret) (2.0.4)\n",
      "Collecting dash-table==5.0.0\n",
      "  Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
      "Collecting dash-core-components==2.0.0\n",
      "  Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from dash>=2.9.0->plotly-resampler>=0.8.3.1->pycaret) (4.3.0)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from dash>=2.9.0->plotly-resampler>=0.8.3.1->pycaret) (1.5.5)\n",
      "Collecting retrying\n",
      "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: Flask<3.1,>=1.0.4 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from dash>=2.9.0->plotly-resampler>=0.8.3.1->pycaret) (1.1.2)\n",
      "Requirement already satisfied: Werkzeug<3.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from dash>=2.9.0->plotly-resampler>=0.8.3.1->pycaret) (2.0.3)\n",
      "Collecting dash-html-components==2.0.0\n",
      "  Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (6.1)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (1.5.1)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (23.2.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (7.3.4)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=5.5.0->pycaret) (0.8.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=4.2.0->pycaret) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=4.2.0->pycaret) (0.18.0)\n",
      "Requirement already satisfied: six in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from patsy>=0.5.1->category-encoders>=2.4.0->pycaret) (1.16.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.5.0->pycaret) (0.2.5)\n",
      "Requirement already satisfied: notebook>=4.4.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.6.5->pycaret) (6.4.12)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from jupyter_core->nbformat>=4.2.0->pycaret) (302)\n",
      "Requirement already satisfied: click>=5.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from Flask<3.1,>=1.0.4->dash>=2.9.0->plotly-resampler>=0.8.3.1->pycaret) (8.0.4)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from Flask<3.1,>=1.0.4->dash>=2.9.0->plotly-resampler>=0.8.3.1->pycaret) (2.0.1)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=7.6.5->pycaret) (0.4)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.5->pycaret) (1.8.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.5->pycaret) (0.14.1)\n",
      "Requirement already satisfied: nbconvert>=5 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.5->pycaret) (6.4.4)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.5->pycaret) (21.3.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.5->pycaret) (0.13.1)\n",
      "Requirement already satisfied: testpath in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.5->pycaret) (0.6.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.5->pycaret) (4.11.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.5->pycaret) (0.5.13)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.5->pycaret) (0.1.2)\n",
      "Requirement already satisfied: bleach in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.5->pycaret) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.5->pycaret) (0.7.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.5->pycaret) (1.5.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.5->pycaret) (0.8.4)\n",
      "Requirement already satisfied: pywinpty>=1.1.0 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.5->pycaret) (2.0.2)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.5->pycaret) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.5->pycaret) (1.15.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.5->pycaret) (2.3.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.5->pycaret) (0.5.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\kevin\\anaconda3\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.6.5->pycaret) (2.21)\n",
      "Building wheels for collected packages: pyod\n",
      "  Building wheel for pyod (setup.py): started\n",
      "  Building wheel for pyod (setup.py): finished with status 'done'\n",
      "  Created wheel for pyod: filename=pyod-2.0.1-py3-none-any.whl size=193273 sha256=e6779cebce346bf851a921c2fb49f3c7fb061a3b794535303579fc0e3d914c79\n",
      "  Stored in directory: c:\\users\\kevin\\appdata\\local\\pip\\cache\\wheels\\23\\71\\88\\c2bab1baecb9b35930a6087879e2e07c5e663c480fea6c0479\n",
      "Successfully built pyod\n",
      "Installing collected packages: kaleido, dash-table, dash-html-components, dash-core-components, xxhash, tsdownsample, scikit-base, schemdraw, retrying, orjson, joblib, jinja2, importlib-metadata, scikit-learn, plotly, lightgbm, deprecation, yellowbrick, sktime, scikit-plot, pyod, imbalanced-learn, dash, pmdarima, plotly-resampler, category-encoders, tbats, pycaret\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.0\n",
      "    Uninstalling joblib-1.1.0:\n",
      "      Successfully uninstalled joblib-1.1.0\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 2.11.3\n",
      "    Uninstalling Jinja2-2.11.3:\n",
      "      Successfully uninstalled Jinja2-2.11.3\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.11.3\n",
      "    Uninstalling importlib-metadata-4.11.3:\n",
      "      Successfully uninstalled importlib-metadata-4.11.3\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "  Attempting uninstall: plotly\n",
      "    Found existing installation: plotly 5.9.0\n",
      "    Uninstalling plotly-5.9.0:\n",
      "      Successfully uninstalled plotly-5.9.0\n",
      "Successfully installed category-encoders-2.6.3 dash-2.17.1 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 deprecation-2.1.0 imbalanced-learn-0.12.3 importlib-metadata-8.2.0 jinja2-3.1.4 joblib-1.3.2 kaleido-0.2.1 lightgbm-4.5.0 orjson-3.10.6 plotly-5.23.0 plotly-resampler-0.10.0 pmdarima-2.0.4 pycaret-3.3.2 pyod-2.0.1 retrying-1.3.4 schemdraw-0.15 scikit-base-0.7.8 scikit-learn-1.4.2 scikit-plot-0.3.7 sktime-0.26.0 tbats-1.1.3 tsdownsample-0.1.3 xxhash-3.4.1 yellowbrick-1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-project 0.11.1 requires ruamel-yaml, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip install pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "669882ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e8467a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a695b",
   "metadata": {},
   "source": [
    "Approach 1 - CountVectorizer with SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5781ec8",
   "metadata": {},
   "source": [
    "Use the CountVectorizer method from the scikit-learn library to define the features from\n",
    "the email text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "305f59c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f08ce9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b65a4333",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, ~df.columns.str.contains('Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a654f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d25b32",
   "metadata": {},
   "source": [
    "Defining the features from email list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cd5a2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['Label','EmailText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b3d5e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>EmailText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                          EmailText\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803ecdf5",
   "metadata": {},
   "source": [
    "Split the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6b67b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform text data into count vector features\n",
    "# Extract features using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(df['EmailText'])\n",
    "y = df['Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f01664",
   "metadata": {},
   "source": [
    "Train an SVM (Support Vector Machine) classifier on the training data, Evaluate the performance of the classifier by calculating accuracy, recall, precision, and\n",
    "F1-score on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b6b5ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.9748878923766816\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline with SVM\n",
    "pipeline = make_pipeline(StandardScaler(with_mean=False), SVC(kernel='linear'))\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "print(f\"Model Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80c2f793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.9748878923766816\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"precision:\", accuracy )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "068148e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.9066666666666667\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "print(\"Recall:\", recall )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24b6a364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score: 0.941379258547137\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the f1 score\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(\"f1-score:\", f1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c483fa",
   "metadata": {},
   "source": [
    "Step 3: Approach 2 - Cleaning Text and SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63582db8",
   "metadata": {},
   "source": [
    "Preprocess the email text by performing cleaning-up techniques such as removing\n",
    "stopwords, punctuation, and performing stemming or lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c108153",
   "metadata": {},
   "source": [
    "Use the cleaned text as features and repeat steps similar to Approach 1: splitting,\n",
    "training an SVM classifier, and evaluating performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e269333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define a function to clean and stem text\n",
    "def clean(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation + '“”'))\n",
    "    # Remove stop words and perform stemming\n",
    "    stemmed_words = [stemmer.stem(word) for word in text.split() if word not in _stop_words.ENGLISH_STOP_WORDS]\n",
    "    # Join words back into a single string\n",
    "    stemmed_text = ' '.join(stemmed_words)\n",
    "    # Remove extra spaces and strip leading/trailing spaces\n",
    "    stemmed_text = ' '.join(stemmed_text.split())\n",
    "    return stemmed_text\n",
    "\n",
    "# Apply the function to the 'EmailText' column\n",
    "df['EmailText'] = df['EmailText'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab3329c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>EmailText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>jurong point crazi avail bugi n great world la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entri 2 wkli comp win fa cup final tkt 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say earli hor u c say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah dont think goe usf live</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                          EmailText\n",
       "0   ham  jurong point crazi avail bugi n great world la...\n",
       "1   ham                              ok lar joke wif u oni\n",
       "2  spam  free entri 2 wkli comp win fa cup final tkt 21...\n",
       "3   ham                        u dun say earli hor u c say\n",
       "4   ham                        nah dont think goe usf live"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a618acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610244fa",
   "metadata": {},
   "source": [
    "training the SVM and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2125f9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.9721973094170404\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['EmailText'])\n",
    "y = df['Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Create a pipeline with SVM\n",
    "pipeline = make_pipeline(StandardScaler(with_mean=False), SVC(kernel='linear'))\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "print(f\"Model Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "703e25ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.9721973094170404\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"precision:\", accuracy )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca7278cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.8966666666666667\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "print(\"Recall:\", recall )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cfc66ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score: 0.9344750516104938\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the f1 score\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(\"f1-score:\", f1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f7305f",
   "metadata": {},
   "source": [
    "Approach 3 - TF-IDF Vectors and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67428f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "# Fit and transform the cleaned text data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['EmailText'])\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de20be7f",
   "metadata": {},
   "source": [
    "Split the data, train an SVM classifier, and evaluate the performance metrics as in the\n",
    "previous approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2aeb426a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8654708520179372\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=0.1, max_df=0.9)\n",
    "\n",
    "# Fit and transform the text data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['EmailText'])\n",
    "\n",
    "# Extract labels\n",
    "y = df['Label']  # Assuming the label column is named 'Label'\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train an SVM classifier\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e3e4edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.9748878923766816\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"precision:\", accuracy )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2662dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.9066666666666667\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "print(\"Recall:\", recall )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a116632e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score: 0.941379258547137\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the f1 score\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(\"f1-score:\", f1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56450ef2",
   "metadata": {},
   "source": [
    "Compare Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d74c35",
   "metadata": {},
   "source": [
    "Compare the accuracy, recall, precision, and F1-score obtained from each approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098a6e37",
   "metadata": {},
   "source": [
    "Model Accuracy: 0.9748878923766816\n",
    "precision: 0.9748878923766816\n",
    "Recall: 0.9066666666666667\n",
    "f1-score: 0.941379258547137\n",
    "\n",
    "\n",
    "Cleaned text\n",
    "Model Accuracy: 0.9721973094170404\n",
    "precision: 0.9721973094170404\n",
    "Recall: 0.8966666666666667\n",
    "f1-score: 0.9344750516104938\n",
    "\n",
    "TF-IDF\n",
    "Model Accuracy: 0.8654708520179372\n",
    "precision: 0.9748878923766816\n",
    "Recall: 0.9066666666666667\n",
    "f1-score: 0.941379258547137"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe39b2f",
   "metadata": {},
   "source": [
    "Due to the slight variation of the text due to the contents being cleaned, there was a slight reduction in the statistics due to the fact that the LLM was initially trained on different data. This is due to the fact that the LLM was trying to predict based on uncleaned text, highlighting the reduction in quality.\n",
    "\n",
    "The TF-IDF model is extroardinarily close to the initial model accuracy due to how the email texts does not have a large quantity of frequent similar terms to influence LLM training but instead, the email texts mostly contain different key phrases. This explains why there was minimum change within the quality of the score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8850bc",
   "metadata": {},
   "source": [
    "Analyze and discuss the results, identifying the most effective approach for spam\n",
    "detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b651d9c",
   "metadata": {},
   "source": [
    "The most effective approach for SPAM detection is approach 2. Where text cleaning and preprocessing has occured before processing. This is due to how it would allow the training of the LLM with properlly cleaned text which allows for more efficient text analysis in a general sense for upcoming email texts and the LLM would not be trained on misspellings. \n",
    "\n",
    "TF-IDF would not be effective due to how email texts would contain a large quantity of variation of words and key phrases that analyzing based on term frequency would be ineffective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b130c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
